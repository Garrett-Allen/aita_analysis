{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import transformers\n",
    "from transformers import  AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# the name of the pre-trained model we want to use\n",
    "MODEL_NAME = \"bert-base-uncased\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"../labeled_post_enriched_bert.parquet\")\n",
    "data = data.drop([\"index\", \"link_flair_text_y\", \"Unnamed: 0\"], axis=1)\n",
    "data = data.rename({\"link_flair_text_x\": \"link_flair_text\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['link_flair_text', 'num_comments', 'over_18', 'score', 'url',\n",
       "       'selftext', 'title', 'id', 'edited', 'is_self', 'permalink', 'downs',\n",
       "       'ups', 'created', 'date', 'Topic', 'Count', 'Name', 'Representation',\n",
       "       'verdict', 'is_asshole', 'demonyms', 'demonyms_words_count',\n",
       "       'demonyms_unique_count', 'features', 'title_uppercase_count',\n",
       "       'title_word_count', 'title_profanity_count', 'avg_word_length',\n",
       "       'stop_word_count', 'numerics_count', 'uppercase_words_count',\n",
       "       'sentence_count', 'avg_sentence_length', 'profanity_count', 'gender',\n",
       "       'age', 'llama_gender', 'llama_age', 'gender_F', 'gender_M', 'gender_U',\n",
       "       'title_anger', 'title_disgust', 'title_fear', 'title_joy',\n",
       "       'title_others', 'title_sadness', 'title_surprise', 'selftext_anger',\n",
       "       'selftext_disgust', 'selftext_fear', 'selftext_joy', 'selftext_others',\n",
       "       'selftext_sadness', 'selftext_surprise'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_parquet(\"../labeled_post.parquet\")\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Not the A-hole     55095\n",
       "Asshole            13142\n",
       "No A-holes here     3470\n",
       "Everyone Sucks      2951\n",
       "Name: link_flair_text, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"link_flair_text\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    58565\n",
       "1    16093\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flairs = {\n",
    "    \"Not the A-hole\": \"NTA\",\n",
    "    \"No A-holes here\": \"NTA\",\n",
    "    \"Asshole\": \"YTA\",\n",
    "    \"Everyone Sucks\": \"YTA\"\n",
    "}\n",
    "\n",
    "# filter data and label verdicts\n",
    "labeled_data = data.loc[data.link_flair_text.isin(flairs)].copy()\n",
    "labeled_data[\"verdict\"] = labeled_data.link_flair_text.map(flairs)\n",
    "labeled_data[\"verdict\"] = (labeled_data[\"verdict\"] == \"YTA\").astype(int)\n",
    "labeled_data[\"verdict\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_data = labeled_data[labeled_data[\"verdict\"] == 0]\n",
    "positive_data = labeled_data[labeled_data[\"verdict\"] == 1]\n",
    "negative_data = negative_data.sample(len(positive_data), random_state=547)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    16093\n",
       "1    16093\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data = pd.concat([negative_data, positive_data])\n",
    "labeled_data[\"verdict\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the textual input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data[\"gender\"] = labeled_data[\"gender\"].map({\"F\": \"Female\", \"M\": \"Male\", \"U\": \"Unknown\"})\n",
    "labeled_data[\"age\"] = labeled_data[\"age\"].fillna(-1).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_cols = ['title_anger', 'title_disgust', 'title_fear', 'title_joy',\n",
    "       'title_others', 'title_sadness', 'title_surprise', 'selftext_anger',\n",
    "       'selftext_disgust', 'selftext_fear', 'selftext_joy', 'selftext_others',\n",
    "       'selftext_sadness', 'selftext_surprise']\n",
    "for col in sentiment_cols:\n",
    "    labeled_data[col] = labeled_data[col].round(6).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features = [\"gender\", \"age\", \"Name\", 'title_anger', 'title_disgust', 'title_fear', 'title_joy',\n",
    "       'title_others', 'title_sadness', 'title_surprise', 'selftext_anger',\n",
    "       'selftext_disgust', 'selftext_fear', 'selftext_joy', 'selftext_others',\n",
    "       'selftext_sadness', 'selftext_surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data[\"input\"] = \"gender: \" + labeled_data[\"gender\"] + \" / age: \" + labeled_data[\"age\"] + \" / topic: \" + labeled_data[\"Name\"] + \\\n",
    "    \" / title anger score: \" + labeled_data[\"title_anger\"] +  \" / title disgust score: \" + labeled_data[\"title_disgust\"] + \\\n",
    "    \" / title fear score: \" + labeled_data[\"title_fear\"] +  \" / title joy score: \" + labeled_data[\"title_joy\"] + \\\n",
    "    \" / title others score: \" + labeled_data[\"title_others\"] +  \" / title sadness score: \" + labeled_data[\"title_sadness\"] + \\\n",
    "    \" / title surprise score: \" + labeled_data[\"title_surprise\"] +  \\\n",
    "    \" / text anger score: \" + labeled_data[\"selftext_anger\"] +  \" / text disgust score: \" + labeled_data[\"selftext_disgust\"] + \\\n",
    "    \" / text fear score: \" + labeled_data[\"selftext_fear\"] +  \" / text joy score: \" + labeled_data[\"selftext_joy\"] + \\\n",
    "    \" / text others score: \" + labeled_data[\"selftext_others\"] +  \" / text sadness score: \" + labeled_data[\"selftext_sadness\"] + \\\n",
    "    \" / text surprise score: \" + labeled_data[\"selftext_surprise\"] + \\\n",
    "    \" / title: \" + labeled_data[\"title\"] + \" / text: \" + labeled_data[\"selftext\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gender: Female / age: 27 / topic: finances mortgage savings rent / title anger score: 0.001632 / title disgust score: 0.003698 / title fear score: 0.001713 / title joy score: 0.012659 / title others score: 0.976187 / title sadness score: 0.00127 / title surprise score: 0.002842 / text anger score: 0.001026 / text disgust score: 0.005979 / text fear score: 0.003277 / text joy score: 0.009596 / text others score: 0.975425 / text sadness score: 0.00285 / text surprise score: 0.001847 / title: AITA for doubting my BF’s plan to start a small business? / text: For context I (27F) am dating my bf (32m) for about 2 years now. We have started to have some larger discussions regarding family planning, finances, family, marriage, etc. you get the gist. \\n\\nMy BF and I have very different backgrounds, we both work in corporate and have great benefits, I am child free, have absolutely no debt, trying to buy my first house, and am doing well for myself generally speaking. My BF has a son from a previous relationship, significant debt, family issues, living month to month, etc. it’s a lot. \\n\\nThrough discussing everything he has mentioned in about 6-10 years he’d want to start a brick and mortar athletic wear store. I’ve gone to business school (big flex I know lol), so I immediately asked questions about it not being profitable for several years, no PTO, working 10- 12 hour days, weekends, having a very sizable business loan, retirement, living off only my income for a long time, no health insurance for him and his son, not a lot of time he’d spend with our future kids… a lot of stuff. This was obviously not received well at all and now I feel like crap for poo-pooing someone’s dream while future planning.\\n\\n\\nTLDR: My bf wants to start a small brick and mortar business and I overwhelmed them with concerns about their debt, finances, future family planning, etc. and now I feel like an AH.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data[\"input\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing import remove_stopwords, strip_numeric, strip_punctuation, strip_multiple_whitespaces\n",
    "\n",
    "# labeled_data[\"title_text\"] = labeled_data[\"title\"] + \" --- \" + labeled_data[\"selftext\"]\n",
    "labeled_data[\"input\"] = labeled_data[\"input\"].str.lower()\n",
    "labeled_data[\"input\"] = labeled_data[\"input\"].apply(strip_multiple_whitespaces)\n",
    "labeled_data = labeled_data.replace(to_replace=[''], value=np.nan).dropna(subset=[\"input\", \"verdict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8157</th>\n",
       "      <td>gender: female / age: 40 / topic: parents upse...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59194</th>\n",
       "      <td>gender: female / age: 27 / topic: finances mor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45681</th>\n",
       "      <td>gender: female / age: 41 / topic: parents upse...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10748</th>\n",
       "      <td>gender: male / age: 18 / topic: friendship tal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35567</th>\n",
       "      <td>gender: female / age: 21 / topic: parents upse...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  verdict\n",
       "8157   gender: female / age: 40 / topic: parents upse...        0\n",
       "59194  gender: female / age: 27 / topic: finances mor...        0\n",
       "45681  gender: female / age: 41 / topic: parents upse...        0\n",
       "10748  gender: male / age: 18 / topic: friendship tal...        0\n",
       "35567  gender: female / age: 21 / topic: parents upse...        0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = labeled_data[[\"input\", \"verdict\"]]\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(final_data, test_size=0.2, random_state=547)\n",
    "train.to_csv(\"./data/train.csv\", index=False)\n",
    "val.to_csv(\"./data/val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-99fe7deac6e319af\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\Wenhao\\.cache\\huggingface\\datasets\\csv\\default-99fe7deac6e319af\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 905.80it/s]\n",
      "                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\Wenhao\\.cache\\huggingface\\datasets\\csv\\default-99fe7deac6e319af\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 332.83it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"./data/train.csv\", \"val\": \"./data/val.csv\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Wenhao/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\Wenhao/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\Wenhao/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Wenhao/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Wenhao/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "### Preprocess Setup ###\n",
    "# tokenization hyperparameters\n",
    "PADDING = 'max_length' # padding strategy\n",
    "PADDING_SIDE = 'right' # the side on which the model should have padding applied\n",
    "TRUNCATION = True # truncate strategy\n",
    "TRUNCATION_SIDE = 'right' # the side on which the model should have truncation applied\n",
    "MAX_LEN = 512 # maximum length to use by one of the truncation/padding parameters\n",
    "\n",
    "# Load the pre-trained tokenmizer ###\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    padding_side=PADDING_SIDE,\n",
    "    truncation_side=TRUNCATION_SIDE,\n",
    ")\n",
    "\n",
    "# Define the preprocess function ###\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Preprocess the description field\n",
    "    ---\n",
    "    Arguments:\n",
    "    examples (str, List[str], List[List[str]]: the sequence or batch of sequences to be encoded/tokenized\n",
    "\n",
    "    Returns:\n",
    "    tokenized (transformers.BatchEncoding): tokenized descriptions \n",
    "    \"\"\"\n",
    "    tokenized = TOKENIZER(\n",
    "        examples[\"input\"],\n",
    "        padding=PADDING,\n",
    "        truncation=TRUNCATION,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:09<00:00,  2.78ba/s]\n",
      "100%|██████████| 7/7 [00:02<00:00,  2.86ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.rename_column(\"verdict\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'input_ids': [101,\n",
       "  5907,\n",
       "  1024,\n",
       "  3287,\n",
       "  1013,\n",
       "  2287,\n",
       "  1024,\n",
       "  1011,\n",
       "  1015,\n",
       "  1013,\n",
       "  8476,\n",
       "  1024,\n",
       "  6860,\n",
       "  2831,\n",
       "  28939,\n",
       "  6314,\n",
       "  1013,\n",
       "  2516,\n",
       "  4963,\n",
       "  3556,\n",
       "  1024,\n",
       "  1014,\n",
       "  1012,\n",
       "  4002,\n",
       "  21926,\n",
       "  2575,\n",
       "  2475,\n",
       "  1013,\n",
       "  2516,\n",
       "  12721,\n",
       "  3556,\n",
       "  1024,\n",
       "  1014,\n",
       "  1012,\n",
       "  4002,\n",
       "  2575,\n",
       "  23352,\n",
       "  2487,\n",
       "  1013,\n",
       "  2516,\n",
       "  3571,\n",
       "  3556,\n",
       "  1024,\n",
       "  1014,\n",
       "  1012,\n",
       "  4002,\n",
       "  21926,\n",
       "  2581,\n",
       "  2575,\n",
       "  1013,\n",
       "  2516,\n",
       "  6569,\n",
       "  3556,\n",
       "  1024,\n",
       "  1014,\n",
       "  1012,\n",
       "  4002,\n",
       "  2620,\n",
       "  21619,\n",
       "  2549,\n",
       "  1013,\n",
       "  2516,\n",
       "  2500,\n",
       "  3556,\n",
       "  1024,\n",
       "  1014,\n",
       "  1012,\n",
       "  5989,\n",
       "  28311,\n",
       "  21926,\n",
       "  1013,\n",
       "  2516,\n",
       "  12039,\n",
       "  3556,\n",
       "  1024,\n",
       "  1014,\n",
       "  1012,\n",
       "  25604,\n",
       "  2692,\n",
       "  2575,\n",
       "  1013,\n",
       "  2516,\n",
       "  4474,\n",
       "  3556,\n",
       "  1024,\n",
       "  1014,\n",
       "  1012,\n",
       "  4002,\n",
       "  22394,\n",
       "  2575,\n",
       "  2509,\n",
       "  1013,\n",
       "  3793,\n",
       "  4963,\n",
       "  3556,\n",
       "  1024,\n",
       "  1014,\n",
       "  1012,\n",
       "  25604,\n",
       "  2692,\n",
       "  22275,\n",
       "  1013,\n",
       "  3793,\n",
       "  12721,\n",
       "  3556,\n",
       "  1024,\n",
       "  1014,\n",
       "  1012,\n",
       "  5890,\n",
       "  18827,\n",
       "  28154,\n",
       "  1013,\n",
       "  3793,\n",
       "  3571,\n",
       "  3556,\n",
       "  1024,\n",
       "  1014,\n",
       "  1012,\n",
       "  4002,\n",
       "  22025,\n",
       "  27814,\n",
       "  1013,\n",
       "  3793,\n",
       "  6569,\n",
       "  3556,\n",
       "  1024,\n",
       "  1014,\n",
       "  1012,\n",
       "  4002,\n",
       "  2620,\n",
       "  23833,\n",
       "  2487,\n",
       "  1013,\n",
       "  3793,\n",
       "  2500,\n",
       "  3556,\n",
       "  1024,\n",
       "  1014,\n",
       "  1012,\n",
       "  5986,\n",
       "  2683,\n",
       "  12740,\n",
       "  2581,\n",
       "  1013,\n",
       "  3793,\n",
       "  12039,\n",
       "  3556,\n",
       "  1024,\n",
       "  1014,\n",
       "  1012,\n",
       "  2199,\n",
       "  2620,\n",
       "  2581,\n",
       "  2487,\n",
       "  1013,\n",
       "  3793,\n",
       "  4474,\n",
       "  3556,\n",
       "  1024,\n",
       "  1014,\n",
       "  1012,\n",
       "  4002,\n",
       "  12740,\n",
       "  2683,\n",
       "  2487,\n",
       "  1013,\n",
       "  2516,\n",
       "  1024,\n",
       "  9932,\n",
       "  2696,\n",
       "  2005,\n",
       "  2437,\n",
       "  1037,\n",
       "  8257,\n",
       "  2007,\n",
       "  2026,\n",
       "  6513,\n",
       "  1029,\n",
       "  1013,\n",
       "  3793,\n",
       "  1024,\n",
       "  7483,\n",
       "  1010,\n",
       "  2026,\n",
       "  6513,\n",
       "  1998,\n",
       "  1045,\n",
       "  2020,\n",
       "  3331,\n",
       "  2055,\n",
       "  6721,\n",
       "  4933,\n",
       "  1998,\n",
       "  1045,\n",
       "  2081,\n",
       "  1037,\n",
       "  7615,\n",
       "  2008,\n",
       "  2016,\n",
       "  6732,\n",
       "  2001,\n",
       "  5805,\n",
       "  2021,\n",
       "  1045,\n",
       "  2228,\n",
       "  2001,\n",
       "  3492,\n",
       "  28378,\n",
       "  1012,\n",
       "  1043,\n",
       "  2546,\n",
       "  1024,\n",
       "  2876,\n",
       "  1521,\n",
       "  1056,\n",
       "  2009,\n",
       "  2022,\n",
       "  4658,\n",
       "  2065,\n",
       "  2045,\n",
       "  2001,\n",
       "  1037,\n",
       "  13272,\n",
       "  2996,\n",
       "  2157,\n",
       "  10025,\n",
       "  2013,\n",
       "  2115,\n",
       "  4545,\n",
       "  1029,\n",
       "  2033,\n",
       "  1024,\n",
       "  4931,\n",
       "  1010,\n",
       "  2065,\n",
       "  2017,\n",
       "  1521,\n",
       "  2128,\n",
       "  7929,\n",
       "  2007,\n",
       "  1037,\n",
       "  9129,\n",
       "  1997,\n",
       "  2308,\n",
       "  1999,\n",
       "  27090,\n",
       "  4253,\n",
       "  22491,\n",
       "  2055,\n",
       "  2648,\n",
       "  2026,\n",
       "  4545,\n",
       "  1010,\n",
       "  2008,\n",
       "  1521,\n",
       "  1055,\n",
       "  2986,\n",
       "  2007,\n",
       "  2033,\n",
       "  1012,\n",
       "  2279,\n",
       "  2518,\n",
       "  1045,\n",
       "  2113,\n",
       "  1010,\n",
       "  2016,\n",
       "  2003,\n",
       "  4129,\n",
       "  2033,\n",
       "  2008,\n",
       "  1045,\n",
       "  2572,\n",
       "  2108,\n",
       "  6881,\n",
       "  1998,\n",
       "  17109,\n",
       "  1998,\n",
       "  2008,\n",
       "  1045,\n",
       "  2572,\n",
       "  2667,\n",
       "  2000,\n",
       "  2191,\n",
       "  2014,\n",
       "  9981,\n",
       "  1012,\n",
       "  2016,\n",
       "  2056,\n",
       "  2008,\n",
       "  2026,\n",
       "  7615,\n",
       "  2001,\n",
       "  7977,\n",
       "  1998,\n",
       "  2008,\n",
       "  2009,\n",
       "  17733,\n",
       "  2014,\n",
       "  1012,\n",
       "  1045,\n",
       "  2409,\n",
       "  2014,\n",
       "  2000,\n",
       "  9483,\n",
       "  2138,\n",
       "  2009,\n",
       "  2001,\n",
       "  1037,\n",
       "  12873,\n",
       "  8257,\n",
       "  2055,\n",
       "  1037,\n",
       "  25613,\n",
       "  3663,\n",
       "  1998,\n",
       "  2008,\n",
       "  1045,\n",
       "  2134,\n",
       "  1521,\n",
       "  1056,\n",
       "  2812,\n",
       "  2000,\n",
       "  2125,\n",
       "  10497,\n",
       "  2014,\n",
       "  1012,\n",
       "  2515,\n",
       "  2026,\n",
       "  7615,\n",
       "  2191,\n",
       "  2033,\n",
       "  1996,\n",
       "  22052,\n",
       "  2030,\n",
       "  2001,\n",
       "  2016,\n",
       "  2058,\n",
       "  16416,\n",
       "  11873,\n",
       "  1029,\n",
       "  4283,\n",
       "  1999,\n",
       "  5083,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import matthews_corrcoef, cohen_kappa_score, confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "### Evaluation Metrics ###\n",
    "metric_acc = load_metric(\"accuracy\")\n",
    "metric_f1 = load_metric(\"f1\")\n",
    "metric_precision = load_metric(\"precision\")\n",
    "metric_recall = load_metric(\"recall\")\n",
    "metric_auc = load_metric(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute the metrics \n",
    "    ---\n",
    "    Arguments:\n",
    "    eval_pred (tuple): the predicted logits and truth labels\n",
    "\n",
    "    Returns:\n",
    "    metrics (dict{str: float}): contains the computed metrics \n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    prediction_scores = np.max(logits, axis=-1)\n",
    "    print(logits.shape, labels.shape)\n",
    "    print(predictions.shape, prediction_scores.shape)\n",
    "\n",
    "    pred_true = np.count_nonzero(predictions)\n",
    "    pred_false = predictions.shape[0] - pred_true\n",
    "    actual_true = np.count_nonzero(labels)\n",
    "    actual_false = labels.shape[0] - actual_true\n",
    "\n",
    "    acc = metric_acc.compute(predictions=predictions, references=labels)['accuracy']\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels)['f1']\n",
    "    precision = metric_precision.compute(predictions=predictions, references=labels)['precision']\n",
    "    recall = metric_recall.compute(predictions=predictions, references=labels)['recall']\n",
    "    roc_auc = metric_auc.compute(prediction_scores=predictions, references=labels)['roc_auc']\n",
    "    matthews_correlation = matthews_corrcoef(y_true=labels, y_pred=predictions)\n",
    "    cohen_kappa = cohen_kappa_score(y1=labels, y2=predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true=labels, y_pred=predictions)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=labels, y_pred=predictions).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    informedness = specificity + sensitivity - 1\n",
    "\n",
    "    metrics = {\n",
    "        \"pred_true\": pred_true,\n",
    "        \"pred_false\": pred_false,\n",
    "        \"actual_true\": actual_true,\n",
    "        \"actual_false\": actual_false,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_score\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"matthews_correlation\": matthews_correlation,\n",
    "        \"cohen_kappa\": cohen_kappa,\n",
    "        \"true_negative\": tn,\n",
    "        \"false_positive\": fp,\n",
    "        \"false_negative\": fn,\n",
    "        \"true_positive\": tp,\n",
    "        \"specificity\": specificity,\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"informedness\": informedness,\n",
    "        \"balanced_accuracy\": balanced_accuracy\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "CLASSIFIER_DROPOUT = 0.15 # dropout ratio for the classification head\n",
    "NUM_CLASSES = 2 # number of classes\n",
    "\n",
    "# optimization hyperparameters ###\n",
    "SEED = 547 # random seed for splitting the data into batches\n",
    "BATCH_SIZE = 16 # batch size for both training and evaluation\n",
    "GRAD_ACC_STEPS = 2 # number of steps for gradient accumulation\n",
    "LR = 5e-5 # initial learning rate\n",
    "WEIGHT_DECAY = 2e-3 # weight decay to apply in the AdamW optimizer\n",
    "EPOCHS = 3 # total number of training epochs \n",
    "LR_SCHEDULER = \"cosine\" # type of learning rate scheduler\n",
    "STRATEGY = \"steps\" # strategy for logging, evaluation, and saving\n",
    "STEPS = 100 # number of steps for logging, evaluation, and saving\n",
    "EVAL_METRIC = \"f1_score\" # metric for selecting the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Wenhao/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": 0.15,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Wenhao/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    classifier_dropout=CLASSIFIER_DROPOUT,\n",
    "    num_labels=NUM_CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "# set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY, \n",
    "    num_train_epochs=EPOCHS,\n",
    "    lr_scheduler_type=LR_SCHEDULER,\n",
    "    evaluation_strategy=STRATEGY,\n",
    "    logging_strategy=STRATEGY, \n",
    "    save_strategy=STRATEGY,\n",
    "    eval_steps=STEPS,\n",
    "    logging_steps=STEPS,\n",
    "    save_steps=STEPS,\n",
    "    seed=SEED,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=EVAL_METRIC,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the trainer \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=TOKENIZER,   \n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree(\"./model\") # remove possible cache\n",
    "    # shutil.rmtree(best_model_dir)\n",
    "    # os.remove(best_model_dir_zip)\n",
    "    os.mkdir(\"./model\")\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 25748\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 2415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 28/2415 [04:00<8:38:39, 13.04s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\transformers\\trainer.py:1498\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1493\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1495\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1497\u001b[0m )\n\u001b[1;32m-> 1498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\transformers\\trainer.py:1740\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1738\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1743\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1744\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1746\u001b[0m ):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\transformers\\trainer.py:2488\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2486\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[0;32m   2487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2488\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./model/first_model\\config.json\n",
      "Model weights saved in ./model/first_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./model/first_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we tune three hyperparameters -- learning rate, weight_decay, dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "# model hyperparameters\n",
    "CLASSIFIER_DROPOUT = 0.15 # dropout ratio for the classification head\n",
    "NUM_CLASSES = 2 # number of classes\n",
    "\n",
    "# optimization hyperparameters ###\n",
    "SEED = 547 # random seed for splitting the data into batches\n",
    "BATCH_SIZE = 32 # batch size for both training and evaluation\n",
    "GRAD_ACC_STEPS = 1 # number of steps for gradient accumulation\n",
    "LR = 5e-5 # initial learning rate\n",
    "WEIGHT_DECAY = 2e-3 # weight decay to apply in the AdamW optimizer\n",
    "EPOCHS = 3 # total number of training epochs \n",
    "LR_SCHEDULER = \"cosine\" # type of learning rate scheduler\n",
    "STRATEGY = \"steps\" # strategy for logging, evaluation, and saving\n",
    "STEPS = 100 # number of steps for logging, evaluation, and saving\n",
    "EVAL_METRIC = \"f1_score\" # metric for selecting the best model\n",
    "\n",
    "HYPERPARAMETER_GRID = {\n",
    "    \"learning_rate\": [5e-5, 3e-5, 2e-5], # recommended by the BERT authors\n",
    "    \"weight_decay\": [1/8 * 1e-3, 1/4 * 1e-3, 1/2 * 1e-3], # recommended by the AdamW authors\n",
    "    \"dropout\": [1e-1, 3e-1, 5e-1]\n",
    "}\n",
    "\n",
    "ALL_COMBINATIONS = it.product(*(HYPERPARAMETER_GRID[key] for key in HYPERPARAMETER_GRID))\n",
    "ALL_COMBINATIONS = list(ALL_COMBINATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Learning rate = 5e-05, Weight decay = 0.000125, Classifier dropout = 0.1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 25748\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2415\n",
      "  4%|▍         | 100/2415 [05:39<3:20:45,  5.20s/it]***** Running Evaluation *****\n",
      "  Num examples = 6438\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.698, 'learning_rate': 4.97887664223157e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                    \n",
      "  4%|▍         | 100/2415 [08:36<3:20:45,  5.20s/it]Saving model checkpoint to ./model\\checkpoint-100\n",
      "Configuration saved in ./model\\checkpoint-100\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6438, 2) (6438,)\n",
      "(6438,) (6438,)\n",
      "{'eval_loss': 0.6930596232414246, 'eval_pred_true': 0, 'eval_pred_false': 6438, 'eval_actual_true': 3177, 'eval_actual_false': 3261, 'eval_accuracy': 0.5065237651444549, 'eval_f1_score': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_roc_auc': 0.5, 'eval_matthews_correlation': 0.0, 'eval_cohen_kappa': 0.0, 'eval_true_negative': 3261, 'eval_false_positive': 0, 'eval_false_negative': 3177, 'eval_true_positive': 0, 'eval_specificity': 1.0, 'eval_sensitivity': 0.0, 'eval_informedness': 0.0, 'eval_balanced_accuracy': 0.5, 'eval_runtime': 177.0984, 'eval_samples_per_second': 36.353, 'eval_steps_per_second': 1.141, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-100\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-100\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-100\\special_tokens_map.json\n",
      "  8%|▊         | 200/2415 [59:20<20:04:56, 32.64s/it]***** Running Evaluation *****\n",
      "  Num examples = 6438\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.698, 'learning_rate': 4.91586352592101e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                     \n",
      "  8%|▊         | 200/2415 [1:59:00<20:04:56, 32.64s/it]Saving model checkpoint to ./model\\checkpoint-200\n",
      "Configuration saved in ./model\\checkpoint-200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6438, 2) (6438,)\n",
      "(6438,) (6438,)\n",
      "{'eval_loss': 0.6935803890228271, 'eval_pred_true': 0, 'eval_pred_false': 6438, 'eval_actual_true': 3177, 'eval_actual_false': 3261, 'eval_accuracy': 0.5065237651444549, 'eval_f1_score': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_roc_auc': 0.5, 'eval_matthews_correlation': 0.0, 'eval_cohen_kappa': 0.0, 'eval_true_negative': 3261, 'eval_false_positive': 0, 'eval_false_negative': 3177, 'eval_true_positive': 0, 'eval_specificity': 1.0, 'eval_sensitivity': 0.0, 'eval_informedness': 0.0, 'eval_balanced_accuracy': 0.5, 'eval_runtime': 3579.6556, 'eval_samples_per_second': 1.798, 'eval_steps_per_second': 0.056, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-200\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-200\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-200\\special_tokens_map.json\n",
      " 12%|█▏        | 300/2415 [3:00:12<25:16:21, 43.02s/it]   ***** Running Evaluation *****\n",
      "  Num examples = 6438\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6952, 'learning_rate': 4.8120254899482665e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                       \n",
      " 12%|█▏        | 300/2415 [3:58:23<25:16:21, 43.02s/it]Saving model checkpoint to ./model\\checkpoint-300\n",
      "Configuration saved in ./model\\checkpoint-300\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6438, 2) (6438,)\n",
      "(6438,) (6438,)\n",
      "{'eval_loss': 0.693131685256958, 'eval_pred_true': 0, 'eval_pred_false': 6438, 'eval_actual_true': 3177, 'eval_actual_false': 3261, 'eval_accuracy': 0.5065237651444549, 'eval_f1_score': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_roc_auc': 0.5, 'eval_matthews_correlation': 0.0, 'eval_cohen_kappa': 0.0, 'eval_true_negative': 3261, 'eval_false_positive': 0, 'eval_false_negative': 3177, 'eval_true_positive': 0, 'eval_specificity': 1.0, 'eval_sensitivity': 0.0, 'eval_informedness': 0.0, 'eval_balanced_accuracy': 0.5, 'eval_runtime': 3491.1992, 'eval_samples_per_second': 1.844, 'eval_steps_per_second': 0.058, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-300\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-300\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-300\\special_tokens_map.json\n",
      " 17%|█▋        | 400/2415 [4:55:47<24:09:37, 43.16s/it]   ***** Running Evaluation *****\n",
      "  Num examples = 6438\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6954, 'learning_rate': 4.669117260700397e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                       \n",
      " 17%|█▋        | 400/2415 [5:54:03<24:09:37, 43.16s/it]Saving model checkpoint to ./model\\checkpoint-400\n",
      "Configuration saved in ./model\\checkpoint-400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6438, 2) (6438,)\n",
      "(6438,) (6438,)\n",
      "{'eval_loss': 0.694370687007904, 'eval_pred_true': 0, 'eval_pred_false': 6438, 'eval_actual_true': 3177, 'eval_actual_false': 3261, 'eval_accuracy': 0.5065237651444549, 'eval_f1_score': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_roc_auc': 0.5, 'eval_matthews_correlation': 0.0, 'eval_cohen_kappa': 0.0, 'eval_true_negative': 3261, 'eval_false_positive': 0, 'eval_false_negative': 3177, 'eval_true_positive': 0, 'eval_specificity': 1.0, 'eval_sensitivity': 0.0, 'eval_informedness': 0.0, 'eval_balanced_accuracy': 0.5, 'eval_runtime': 3495.5963, 'eval_samples_per_second': 1.842, 'eval_steps_per_second': 0.058, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-400\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-400\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-400\\special_tokens_map.json\n",
      " 21%|██        | 500/2415 [6:54:33<18:26:13, 34.66s/it]   ***** Running Evaluation *****\n",
      "  Num examples = 6438\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.696, 'learning_rate': 4.489553799500966e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                       \n",
      " 21%|██        | 500/2415 [7:45:55<18:26:13, 34.66s/it]Saving model checkpoint to ./model\\checkpoint-500\n",
      "Configuration saved in ./model\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6438, 2) (6438,)\n",
      "(6438,) (6438,)\n",
      "{'eval_loss': 0.6930409669876099, 'eval_pred_true': 0, 'eval_pred_false': 6438, 'eval_actual_true': 3177, 'eval_actual_false': 3261, 'eval_accuracy': 0.5065237651444549, 'eval_f1_score': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_roc_auc': 0.5, 'eval_matthews_correlation': 0.0, 'eval_cohen_kappa': 0.0, 'eval_true_negative': 3261, 'eval_false_positive': 0, 'eval_false_negative': 3177, 'eval_true_positive': 0, 'eval_specificity': 1.0, 'eval_sensitivity': 0.0, 'eval_informedness': 0.0, 'eval_balanced_accuracy': 0.5, 'eval_runtime': 3081.879, 'eval_samples_per_second': 2.089, 'eval_steps_per_second': 0.066, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-500\\special_tokens_map.json\n",
      " 25%|██▍       | 600/2415 [8:47:57<19:07:47, 37.94s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 6438\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6966, 'learning_rate': 4.2763694929364166e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                       \n",
      " 25%|██▍       | 600/2415 [9:43:56<19:07:47, 37.94s/it]Saving model checkpoint to ./model\\checkpoint-600\n",
      "Configuration saved in ./model\\checkpoint-600\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6438, 2) (6438,)\n",
      "(6438,) (6438,)\n",
      "{'eval_loss': 0.6978518962860107, 'eval_pred_true': 0, 'eval_pred_false': 6438, 'eval_actual_true': 3177, 'eval_actual_false': 3261, 'eval_accuracy': 0.5065237651444549, 'eval_f1_score': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_roc_auc': 0.5, 'eval_matthews_correlation': 0.0, 'eval_cohen_kappa': 0.0, 'eval_true_negative': 3261, 'eval_false_positive': 0, 'eval_false_negative': 3177, 'eval_true_positive': 0, 'eval_specificity': 1.0, 'eval_sensitivity': 0.0, 'eval_informedness': 0.0, 'eval_balanced_accuracy': 0.5, 'eval_runtime': 3358.8279, 'eval_samples_per_second': 1.917, 'eval_steps_per_second': 0.06, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-600\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-600\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-600\\special_tokens_map.json\n",
      " 29%|██▉       | 700/2415 [10:43:26<21:21:55, 44.85s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 6438\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6968, 'learning_rate': 4.033166875709291e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 29%|██▉       | 700/2415 [11:37:03<21:21:55, 44.85s/it]Saving model checkpoint to ./model\\checkpoint-700\n",
      "Configuration saved in ./model\\checkpoint-700\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6438, 2) (6438,)\n",
      "(6438,) (6438,)\n",
      "{'eval_loss': 0.6933590173721313, 'eval_pred_true': 6438, 'eval_pred_false': 0, 'eval_actual_true': 3177, 'eval_actual_false': 3261, 'eval_accuracy': 0.4934762348555452, 'eval_f1_score': 0.6608424336973479, 'eval_precision': 0.4934762348555452, 'eval_recall': 1.0, 'eval_roc_auc': 0.5, 'eval_matthews_correlation': 0.0, 'eval_cohen_kappa': 0.0, 'eval_true_negative': 0, 'eval_false_positive': 3261, 'eval_false_negative': 0, 'eval_true_positive': 3177, 'eval_specificity': 0.0, 'eval_sensitivity': 1.0, 'eval_informedness': 0.0, 'eval_balanced_accuracy': 0.5, 'eval_runtime': 3217.1583, 'eval_samples_per_second': 2.001, 'eval_steps_per_second': 0.063, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-700\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-700\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-700\\special_tokens_map.json\n",
      " 33%|███▎      | 800/2415 [12:10:41<7:24:44, 16.52s/it]    ***** Running Evaluation *****\n",
      "  Num examples = 6438\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6946, 'learning_rate': 3.764055752534714e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 33%|███▎      | 800/2415 [12:18:17<7:24:44, 16.52s/it]Saving model checkpoint to ./model\\checkpoint-800\n",
      "Configuration saved in ./model\\checkpoint-800\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6438, 2) (6438,)\n",
      "(6438,) (6438,)\n",
      "{'eval_loss': 0.7012410163879395, 'eval_pred_true': 6438, 'eval_pred_false': 0, 'eval_actual_true': 3177, 'eval_actual_false': 3261, 'eval_accuracy': 0.4934762348555452, 'eval_f1_score': 0.6608424336973479, 'eval_precision': 0.4934762348555452, 'eval_recall': 1.0, 'eval_roc_auc': 0.5, 'eval_matthews_correlation': 0.0, 'eval_cohen_kappa': 0.0, 'eval_true_negative': 0, 'eval_false_positive': 3261, 'eval_false_negative': 0, 'eval_true_positive': 3177, 'eval_specificity': 0.0, 'eval_sensitivity': 1.0, 'eval_informedness': 0.0, 'eval_balanced_accuracy': 0.5, 'eval_runtime': 455.7198, 'eval_samples_per_second': 14.127, 'eval_steps_per_second': 0.443, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-800\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-800\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-800\\special_tokens_map.json\n",
      " 35%|███▌      | 849/2415 [12:29:59<15:50:49, 36.43s/it] "
     ]
    }
   ],
   "source": [
    "val_eval = {}\n",
    "\n",
    "for combination in ALL_COMBINATIONS:\n",
    "    try:\n",
    "        shutil.rmtree(\"./model\") # remove possible cache\n",
    "        # shutil.rmtree(best_model_dir)\n",
    "        # os.remove(best_model_dir_zip)\n",
    "        os.mkdir(\"./model\")\n",
    "    except:\n",
    "        None\n",
    "\n",
    "    LR = combination[0]\n",
    "    WEIGHT_DECAY = combination[1]\n",
    "    CLASSIFIER_DROPOUT = combination[2]\n",
    "\n",
    "    print(f\"=== Learning rate = {LR}, Weight decay = {WEIGHT_DECAY}, Classifier dropout = {CLASSIFIER_DROPOUT} ===\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        classifier_dropout=CLASSIFIER_DROPOUT,\n",
    "        num_labels=NUM_CLASSES,\n",
    "    )\n",
    "\n",
    "    # set up the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./model\",\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY, \n",
    "        num_train_epochs=EPOCHS,\n",
    "        lr_scheduler_type=LR_SCHEDULER,\n",
    "        evaluation_strategy=STRATEGY,\n",
    "        logging_strategy=STRATEGY, \n",
    "        save_strategy=STRATEGY,\n",
    "        eval_steps=STEPS,\n",
    "        logging_steps=STEPS,\n",
    "        save_steps=STEPS,\n",
    "        seed=SEED,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=EVAL_METRIC,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    # set up the trainer \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['val'],\n",
    "        tokenizer=TOKENIZER,   \n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    val_predictions = trainer.predict(tokenized_dataset[\"val\"])\n",
    "    val_eval[combination] = val_predictions.metrics\n",
    "\n",
    "    val_eval_df = pd.DataFrame.from_dict(val_eval).transpose()\n",
    "    val_eval_df.to_csv(f\"metrics/val_evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_wa_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
