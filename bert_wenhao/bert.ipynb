{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import transformers\n",
    "from transformers import  AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# the name of the pre-trained model we want to use\n",
    "MODEL_NAME = \"bert-base-uncased\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80571, 15)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_parquet(\"../labeled_post.parquet\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    57633\n",
       "1    14135\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flairs = {\n",
    "    'Not the A-hole': \"NTA\",\n",
    "    'Asshole': \"YTA\",\n",
    "#     'No A-holes here': \"NAH\",\n",
    "#     'Everyone Sucks': \"ESH\",\n",
    "#     'Not enough info': \"INFO\",\n",
    "    # 'UPDATE': \"?\",\n",
    "    # 'TL;DR': \"?\",\n",
    "    'not the a-hole': \"NTA\",\n",
    "    # 'POO Mode Activated ðŸ’©',\n",
    "    'asshole': \"YTA\",\n",
    "    # '': \"?\",\n",
    "    # 'META': \"?\",\n",
    "#     'Shitpost': \"?\",\n",
    "    'not the asshole': \"NTA\",\n",
    "#     'no a--holes here': \"NAH\",\n",
    "#     'everyone sucks': \"ESH\",\n",
    "#     'too close to call': \"?\",\n",
    "#     'not enough info': \"INFO\",\n",
    "}\n",
    "\n",
    "# filter data and label verdicts\n",
    "labeled_data = data.loc[data.link_flair_text.isin(flairs)].copy()\n",
    "labeled_data[\"verdict\"] = labeled_data.link_flair_text.map(flairs)\n",
    "labeled_data[\"verdict\"] = (labeled_data[\"verdict\"] == \"YTA\").astype(int)\n",
    "labeled_data[\"verdict\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_data = labeled_data[labeled_data[\"verdict\"] == 0]\n",
    "positive_data = labeled_data[labeled_data[\"verdict\"] == 1]\n",
    "negative_data = negative_data.sample(len(positive_data), random_state=547)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = pd.concat([negative_data, positive_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the textual input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing import remove_stopwords, strip_numeric, strip_punctuation, strip_multiple_whitespaces\n",
    "\n",
    "labeled_data[\"title_text\"] = labeled_data[\"title\"] + \" --- \" + labeled_data[\"selftext\"]\n",
    "labeled_data[\"title_text\"] = labeled_data[\"title_text\"].str.lower()\n",
    "labeled_data[\"title_text\"] = labeled_data[\"title_text\"].apply(strip_multiple_whitespaces)\n",
    "labeled_data = labeled_data.replace(to_replace=[''], value=np.nan).dropna(subset=[\"title_text\", \"verdict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_text</th>\n",
       "      <th>verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15509</th>\n",
       "      <td>aita for not stopping my little brother from d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54638</th>\n",
       "      <td>aita for setting my little brother into a fit ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21845</th>\n",
       "      <td>wibta if i called out my landlord? --- i (29f)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13497</th>\n",
       "      <td>aita for not feeding my cousinâ€™s dog? --- my f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26178</th>\n",
       "      <td>aita for telling my step dad i no longer want ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title_text  verdict\n",
       "15509  aita for not stopping my little brother from d...        0\n",
       "54638  aita for setting my little brother into a fit ...        0\n",
       "21845  wibta if i called out my landlord? --- i (29f)...        0\n",
       "13497  aita for not feeding my cousinâ€™s dog? --- my f...        0\n",
       "26178  aita for telling my step dad i no longer want ...        0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = labeled_data[[\"title_text\", \"verdict\"]]\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(final_data, test_size=0.2, random_state=547)\n",
    "train.to_csv(\"./data/train.csv\", index=False)\n",
    "val.to_csv(\"./data/val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-db78e356e0e09e10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\Wenhao\\.cache\\huggingface\\datasets\\csv\\default-db78e356e0e09e10\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 443.65it/s]\n",
      "                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\Wenhao\\.cache\\huggingface\\datasets\\csv\\default-db78e356e0e09e10\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 363.44it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"./data/train.csv\", \"val\": \"./data/val.csv\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "### Preprocess Setup ###\n",
    "# tokenization hyperparameters\n",
    "PADDING = 'max_length' # padding strategy\n",
    "PADDING_SIDE = 'right' # the side on which the model should have padding applied\n",
    "TRUNCATION = True # truncate strategy\n",
    "TRUNCATION_SIDE = 'right' # the side on which the model should have truncation applied\n",
    "MAX_LEN = 512 # maximum length to use by one of the truncation/padding parameters\n",
    "\n",
    "# Load the pre-trained tokenmizer ###\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    padding_side=PADDING_SIDE,\n",
    "    truncation_side=TRUNCATION_SIDE,\n",
    ")\n",
    "\n",
    "# Define the preprocess function ###\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Preprocess the description field\n",
    "    ---\n",
    "    Arguments:\n",
    "    examples (str, List[str], List[List[str]]: the sequence or batch of sequences to be encoded/tokenized\n",
    "\n",
    "    Returns:\n",
    "    tokenized (transformers.BatchEncoding): tokenized descriptions \n",
    "    \"\"\"\n",
    "    tokenized = TOKENIZER(\n",
    "        examples[\"title_text\"],\n",
    "        padding=PADDING,\n",
    "        truncation=TRUNCATION,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x000001BD94CEDB40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:05<00:00,  4.19ba/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  4.35ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\"title_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.rename_column(\"verdict\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import matthews_corrcoef, cohen_kappa_score, confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "### Evaluation Metrics ###\n",
    "metric_acc = load_metric(\"accuracy\")\n",
    "metric_f1 = load_metric(\"f1\")\n",
    "metric_precision = load_metric(\"precision\")\n",
    "metric_recall = load_metric(\"recall\")\n",
    "metric_auc = load_metric(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute the metrics \n",
    "    ---\n",
    "    Arguments:\n",
    "    eval_pred (tuple): the predicted logits and truth labels\n",
    "\n",
    "    Returns:\n",
    "    metrics (dict{str: float}): contains the computed metrics \n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    prediction_scores = np.max(logits, axis=-1)\n",
    "    print(logits.shape, labels.shape)\n",
    "    print(predictions.shape, prediction_scores.shape)\n",
    "\n",
    "    pred_true = np.count_nonzero(predictions)\n",
    "    pred_false = predictions.shape[0] - pred_true\n",
    "    actual_true = np.count_nonzero(labels)\n",
    "    actual_false = labels.shape[0] - actual_true\n",
    "\n",
    "    acc = metric_acc.compute(predictions=predictions, references=labels)['accuracy']\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels)['f1']\n",
    "    precision = metric_precision.compute(predictions=predictions, references=labels)['precision']\n",
    "    recall = metric_recall.compute(predictions=predictions, references=labels)['recall']\n",
    "    roc_auc = metric_auc.compute(prediction_scores=predictions, references=labels)['roc_auc']\n",
    "    matthews_correlation = matthews_corrcoef(y_true=labels, y_pred=predictions)\n",
    "    cohen_kappa = cohen_kappa_score(y1=labels, y2=predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true=labels, y_pred=predictions)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=labels, y_pred=predictions).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    informedness = specificity + sensitivity - 1\n",
    "\n",
    "    metrics = {\n",
    "        \"pred_true\": pred_true,\n",
    "        \"pred_false\": pred_false,\n",
    "        \"actual_true\": actual_true,\n",
    "        \"actual_false\": actual_false,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_score\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"matthews_correlation\": matthews_correlation,\n",
    "        \"cohen_kappa\": cohen_kappa,\n",
    "        \"true_negative\": tn,\n",
    "        \"false_positive\": fp,\n",
    "        \"false_negative\": fn,\n",
    "        \"true_positive\": tp,\n",
    "        \"specificity\": specificity,\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"informedness\": informedness,\n",
    "        \"balanced_accuracy\": balanced_accuracy\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "CLASSIFIER_DROPOUT = 0.15 # dropout ratio for the classification head\n",
    "NUM_CLASSES = 2 # number of classes\n",
    "\n",
    "# optimization hyperparameters ###\n",
    "SEED = 547 # random seed for splitting the data into batches\n",
    "BATCH_SIZE = 16 # batch size for both training and evaluation\n",
    "GRAD_ACC_STEPS = 4 # number of steps for gradient accumulation\n",
    "LR = 5e-5 # initial learning rate\n",
    "WEIGHT_DECAY = 2e-3 # weight decay to apply in the AdamW optimizer\n",
    "EPOCHS = 10 # total number of training epochs \n",
    "LR_SCHEDULER = \"cosine\" # type of learning rate scheduler\n",
    "STRATEGY = \"steps\" # strategy for logging, evaluation, and saving\n",
    "STEPS = 200 # number of steps for logging, evaluation, and saving\n",
    "EVAL_METRIC = \"f1_score\" # metric for selecting the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    classifier_dropout=CLASSIFIER_DROPOUT,\n",
    "    num_labels=NUM_CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY, \n",
    "    num_train_epochs=EPOCHS,\n",
    "    lr_scheduler_type=LR_SCHEDULER,\n",
    "    evaluation_strategy=STRATEGY,\n",
    "    logging_strategy=STRATEGY, \n",
    "    save_strategy=STRATEGY,\n",
    "    eval_steps=STEPS,\n",
    "    logging_steps=STEPS,\n",
    "    save_steps=STEPS,\n",
    "    seed=SEED,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=EVAL_METRIC,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the trainer \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=TOKENIZER,   \n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree(\"./model\") # remove possible cache\n",
    "    # shutil.rmtree(best_model_dir)\n",
    "    # os.remove(best_model_dir_zip)\n",
    "    os.mkdir(\"./model\")\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wenhao\\miniconda3\\envs\\deep_wa_bert\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 22616\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 3530\n",
      "  6%|â–Œ         | 200/3530 [04:34<1:17:21,  1.39s/it]***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6919, 'learning_rate': 4.960502152176574e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  6%|â–Œ         | 200/3530 [05:20<1:17:21,  1.39s/it]Saving model checkpoint to ./model\\checkpoint-200\n",
      "Configuration saved in ./model\\checkpoint-200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 0.6858039498329163, 'eval_pred_true': 1531, 'eval_pred_false': 4123, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.5680933852140078, 'eval_f1_score': 0.4365482233502538, 'eval_precision': 0.6178967994774657, 'eval_recall': 0.33749554049232966, 'eval_roc_auc': 0.566152189748094, 'eval_matthews_correlation': 0.14886414763463218, 'eval_cohen_kappa': 0.1328117374769574, 'eval_true_negative': 2266, 'eval_false_positive': 585, 'eval_false_negative': 1857, 'eval_true_positive': 946, 'eval_specificity': 0.7948088390038583, 'eval_sensitivity': 0.33749554049232966, 'eval_informedness': 0.132304379496188, 'eval_balanced_accuracy': 0.566152189748094, 'eval_runtime': 45.5841, 'eval_samples_per_second': 124.034, 'eval_steps_per_second': 7.766, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-200\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-200\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-200\\special_tokens_map.json\n",
      " 11%|â–ˆâ–        | 400/3530 [10:06<1:14:32,  1.43s/it] ***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6639, 'learning_rate': 4.843256672692441e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 11%|â–ˆâ–        | 400/3530 [10:53<1:14:32,  1.43s/it]Saving model checkpoint to ./model\\checkpoint-400\n",
      "Configuration saved in ./model\\checkpoint-400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 0.6469425559043884, 'eval_pred_true': 1915, 'eval_pred_false': 3739, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.6183233109303148, 'eval_f1_score': 0.5426027977956762, 'eval_precision': 0.6684073107049608, 'eval_recall': 0.45665358544416695, 'eval_roc_auc': 0.6169623591899894, 'eval_matthews_correlation': 0.247129249974427, 'eval_cohen_kappa': 0.23455023631622474, 'eval_true_negative': 2216, 'eval_false_positive': 635, 'eval_false_negative': 1523, 'eval_true_positive': 1280, 'eval_specificity': 0.777271132935812, 'eval_sensitivity': 0.45665358544416695, 'eval_informedness': 0.23392471837997886, 'eval_balanced_accuracy': 0.6169623591899894, 'eval_runtime': 47.2955, 'eval_samples_per_second': 119.546, 'eval_steps_per_second': 7.485, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-400\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-400\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-400\\special_tokens_map.json\n",
      " 17%|â–ˆâ–‹        | 600/3530 [15:48<1:11:39,  1.47s/it] ***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6295, 'learning_rate': 4.65196831683292e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 17%|â–ˆâ–‹        | 600/3530 [16:36<1:11:39,  1.47s/it]Saving model checkpoint to ./model\\checkpoint-600\n",
      "Configuration saved in ./model\\checkpoint-600\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6593964695930481, 'eval_pred_true': 3525, 'eval_pred_false': 2129, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.6370711001061196, 'eval_f1_score': 0.6757269279393172, 'eval_precision': 0.6065248226950355, 'eval_recall': 0.7627541919372102, 'eval_roc_auc': 0.638129112804803, 'eval_matthews_correlation': 0.28507424591388675, 'eval_cohen_kappa': 0.27566049890112987, 'eval_true_negative': 1464, 'eval_false_positive': 1387, 'eval_false_negative': 665, 'eval_true_positive': 2138, 'eval_specificity': 0.5135040336723956, 'eval_sensitivity': 0.7627541919372102, 'eval_informedness': 0.2762582256096058, 'eval_balanced_accuracy': 0.6381291128048029, 'eval_runtime': 48.5757, 'eval_samples_per_second': 116.396, 'eval_steps_per_second': 7.288, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-600\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-600\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-600\\special_tokens_map.json\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 800/3530 [21:31<1:05:09,  1.43s/it] ***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5883, 'learning_rate': 4.392681467294117e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 800/3530 [22:19<1:05:09,  1.43s/it]Saving model checkpoint to ./model\\checkpoint-800\n",
      "Configuration saved in ./model\\checkpoint-800\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 0.6540129780769348, 'eval_pred_true': 2844, 'eval_pred_false': 2810, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.6439688715953308, 'eval_f1_score': 0.6435275367451744, 'eval_precision': 0.6388888888888888, 'eval_recall': 0.6482340349625402, 'eval_roc_auc': 0.6440047761624346, 'eval_matthews_correlation': 0.2880043807027958, 'eval_cohen_kappa': 0.2879740931845092, 'eval_true_negative': 1824, 'eval_false_positive': 1027, 'eval_false_negative': 986, 'eval_true_positive': 1817, 'eval_specificity': 0.639775517362329, 'eval_sensitivity': 0.6482340349625402, 'eval_informedness': 0.28800955232486913, 'eval_balanced_accuracy': 0.6440047761624346, 'eval_runtime': 47.9635, 'eval_samples_per_second': 117.881, 'eval_steps_per_second': 7.381, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-800\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-800\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-800\\special_tokens_map.json\n",
      " 28%|â–ˆâ–ˆâ–Š       | 1000/3530 [27:14<1:00:49,  1.44s/it]***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5162, 'learning_rate': 4.073589142096592e-05, 'epoch': 2.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 28%|â–ˆâ–ˆâ–Š       | 1000/3530 [28:01<1:00:49,  1.44s/it]Saving model checkpoint to ./model\\checkpoint-1000\n",
      "Configuration saved in ./model\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 0.6895701885223389, 'eval_pred_true': 3096, 'eval_pred_false': 2558, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.6457375309515387, 'eval_f1_score': 0.6604509238854044, 'eval_precision': 0.6291989664082688, 'eval_recall': 0.6949696753478416, 'eval_roc_auc': 0.6461519720127493, 'eval_matthews_correlation': 0.2936257146064592, 'eval_cohen_kappa': 0.29204695697944016, 'eval_true_negative': 1703, 'eval_false_positive': 1148, 'eval_false_negative': 855, 'eval_true_positive': 1948, 'eval_specificity': 0.5973342686776569, 'eval_sensitivity': 0.6949696753478416, 'eval_informedness': 0.29230394402549864, 'eval_balanced_accuracy': 0.6461519720127493, 'eval_runtime': 47.7074, 'eval_samples_per_second': 118.514, 'eval_steps_per_second': 7.42, 'epoch': 2.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-1000\\special_tokens_map.json\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 1200/3530 [32:58<56:47,  1.46s/it]   ***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4, 'learning_rate': 3.704774109322166e-05, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 1200/3530 [33:47<56:47,  1.46s/it]Saving model checkpoint to ./model\\checkpoint-1200\n",
      "Configuration saved in ./model\\checkpoint-1200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 0.9058732390403748, 'eval_pred_true': 3419, 'eval_pred_false': 2235, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.629642730810046, 'eval_f1_score': 0.6634522661523626, 'eval_precision': 0.6036852880959345, 'eval_recall': 0.7363539065287192, 'eval_roc_auc': 0.6305410360423322, 'eval_matthews_correlation': 0.2669923969146744, 'eval_cohen_kappa': 0.260599962253956, 'eval_true_negative': 1496, 'eval_false_positive': 1355, 'eval_false_negative': 739, 'eval_true_positive': 2064, 'eval_specificity': 0.5247281655559453, 'eval_sensitivity': 0.7363539065287192, 'eval_informedness': 0.2610820720846645, 'eval_balanced_accuracy': 0.6305410360423322, 'eval_runtime': 48.3046, 'eval_samples_per_second': 117.049, 'eval_steps_per_second': 7.328, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-1200\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-1200\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-1200\\special_tokens_map.json\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1400/3530 [38:41<50:04,  1.41s/it]   ***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3397, 'learning_rate': 3.297890289002451e-05, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1400/3530 [39:29<50:04,  1.41s/it]Saving model checkpoint to ./model\\checkpoint-1400\n",
      "Configuration saved in ./model\\checkpoint-1400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 0.9191586375236511, 'eval_pred_true': 2889, 'eval_pred_false': 2765, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.6423770781747435, 'eval_f1_score': 0.6447645818692902, 'eval_precision': 0.6351678781585324, 'eval_recall': 0.6546557260078487, 'eval_roc_auc': 0.6424804410467163, 'eval_matthews_correlation': 0.2850191662088481, 'eval_cohen_kappa': 0.28488730163833564, 'eval_true_negative': 1797, 'eval_false_positive': 1054, 'eval_false_negative': 968, 'eval_true_positive': 1835, 'eval_specificity': 0.630305156085584, 'eval_sensitivity': 0.6546557260078487, 'eval_informedness': 0.28496088209343284, 'eval_balanced_accuracy': 0.6424804410467164, 'eval_runtime': 48.3583, 'eval_samples_per_second': 116.919, 'eval_steps_per_second': 7.32, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-1400\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-1400\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-1400\\special_tokens_map.json\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1600/3530 [44:35<49:44,  1.55s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1987, 'learning_rate': 2.8657945093108886e-05, 'epoch': 4.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1600/3530 [45:24<49:44,  1.55s/it]Saving model checkpoint to ./model\\checkpoint-1600\n",
      "Configuration saved in ./model\\checkpoint-1600\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 1.236769437789917, 'eval_pred_true': 2949, 'eval_pred_false': 2705, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.631411390166254, 'eval_f1_score': 0.6376912378303199, 'eval_precision': 0.6219057307561885, 'eval_recall': 0.6542989653942205, 'eval_roc_auc': 0.6316040600383941, 'eval_matthews_correlation': 0.2634440655042421, 'eval_cohen_kappa': 0.2630927607585727, 'eval_true_negative': 1736, 'eval_false_positive': 1115, 'eval_false_negative': 969, 'eval_true_positive': 1834, 'eval_specificity': 0.6089091546825676, 'eval_sensitivity': 0.6542989653942205, 'eval_informedness': 0.2632081200767882, 'eval_balanced_accuracy': 0.6316040600383941, 'eval_runtime': 49.9022, 'eval_samples_per_second': 113.302, 'eval_steps_per_second': 7.094, 'epoch': 4.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-1600\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-1600\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-1600\\special_tokens_map.json\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1800/3530 [50:37<44:00,  1.53s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1748, 'learning_rate': 2.422140252928601e-05, 'epoch': 5.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1800/3530 [51:28<44:00,  1.53s/it]Saving model checkpoint to ./model\\checkpoint-1800\n",
      "Configuration saved in ./model\\checkpoint-1800\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 1.4957462549209595, 'eval_pred_true': 2961, 'eval_pred_false': 2693, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.6268128758401132, 'eval_f1_score': 0.6339347675225537, 'eval_precision': 0.6170212765957447, 'eval_recall': 0.6518016410988227, 'eval_roc_auc': 0.6270232337377664, 'eval_matthews_correlation': 0.2543231754805003, 'eval_cohen_kappa': 0.2539259763268834, 'eval_true_negative': 1717, 'eval_false_positive': 1134, 'eval_false_negative': 976, 'eval_true_positive': 1827, 'eval_specificity': 0.60224482637671, 'eval_sensitivity': 0.6518016410988227, 'eval_informedness': 0.2540464674755327, 'eval_balanced_accuracy': 0.6270232337377664, 'eval_runtime': 50.5751, 'eval_samples_per_second': 111.794, 'eval_steps_per_second': 6.999, 'epoch': 5.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-1800\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-1800\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-1800\\special_tokens_map.json\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2000/3530 [56:25<40:23,  1.58s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0973, 'learning_rate': 1.9809462304994314e-05, 'epoch': 5.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2000/3530 [57:14<40:23,  1.58s/it]Saving model checkpoint to ./model\\checkpoint-2000\n",
      "Configuration saved in ./model\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 1.5220891237258911, 'eval_pred_true': 2821, 'eval_pred_false': 2833, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.6204457021577644, 'eval_f1_score': 0.6184210526315789, 'eval_precision': 0.6164480680609713, 'eval_recall': 0.6204067070995362, 'eval_roc_auc': 0.6204453738935072, 'eval_matthews_correlation': 0.24088260933963873, 'eval_cohen_kappa': 0.24087772631520998, 'eval_true_negative': 1769, 'eval_false_positive': 1082, 'eval_false_negative': 1064, 'eval_true_positive': 1739, 'eval_specificity': 0.6204840406874781, 'eval_sensitivity': 0.6204067070995362, 'eval_informedness': 0.24089074778701436, 'eval_balanced_accuracy': 0.6204453738935072, 'eval_runtime': 49.1839, 'eval_samples_per_second': 114.956, 'eval_steps_per_second': 7.197, 'epoch': 5.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-2000\\special_tokens_map.json\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2200/3530 [1:02:08<33:23,  1.51s/it]***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0751, 'learning_rate': 1.5561534135101884e-05, 'epoch': 6.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2200/3530 [1:02:57<33:23,  1.51s/it]Saving model checkpoint to ./model\\checkpoint-2200\n",
      "Configuration saved in ./model\\checkpoint-2200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 1.9584472179412842, 'eval_pred_true': 2594, 'eval_pred_false': 3060, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.6206225680933852, 'eval_f1_score': 0.6025569760978322, 'eval_precision': 0.6268311488049345, 'eval_recall': 0.5800927577595434, 'eval_roc_auc': 0.6202813841410835, 'eval_matthews_correlation': 0.24137532480661578, 'eval_cohen_kappa': 0.24071385918225097, 'eval_true_negative': 1883, 'eval_false_positive': 968, 'eval_false_negative': 1177, 'eval_true_positive': 1626, 'eval_specificity': 0.6604700105226237, 'eval_sensitivity': 0.5800927577595434, 'eval_informedness': 0.24056276828216694, 'eval_balanced_accuracy': 0.6202813841410835, 'eval_runtime': 48.4612, 'eval_samples_per_second': 116.671, 'eval_steps_per_second': 7.305, 'epoch': 6.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-2200\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-2200\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-2200\\special_tokens_map.json\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2400/3530 [1:07:49<27:27,  1.46s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0594, 'learning_rate': 1.1611845235944144e-05, 'epoch': 6.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2400/3530 [1:08:38<27:27,  1.46s/it]Saving model checkpoint to ./model\\checkpoint-2400\n",
      "Configuration saved in ./model\\checkpoint-2400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 1.999822735786438, 'eval_pred_true': 2621, 'eval_pred_false': 3033, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.6222143615139724, 'eval_f1_score': 0.6061946902654867, 'eval_precision': 0.6272415108737123, 'eval_recall': 0.5865144488048519, 'eval_roc_auc': 0.6219138361176136, 'eval_matthews_correlation': 0.24446879647095257, 'eval_cohen_kappa': 0.24396101908292056, 'eval_true_negative': 1874, 'eval_false_positive': 977, 'eval_false_negative': 1159, 'eval_true_positive': 1644, 'eval_specificity': 0.6573132234303753, 'eval_sensitivity': 0.5865144488048519, 'eval_informedness': 0.24382767223522728, 'eval_balanced_accuracy': 0.6219138361176136, 'eval_runtime': 48.5263, 'eval_samples_per_second': 116.514, 'eval_steps_per_second': 7.295, 'epoch': 6.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-2400\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-2400\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-2400\\special_tokens_map.json\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2600/3530 [1:13:34<22:30,  1.45s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0425, 'learning_rate': 8.085198976392127e-06, 'epoch': 7.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2600/3530 [1:14:23<22:30,  1.45s/it]Saving model checkpoint to ./model\\checkpoint-2600\n",
      "Configuration saved in ./model\\checkpoint-2600\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 2.142937421798706, 'eval_pred_true': 2973, 'eval_pred_false': 2681, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.622568093385214, 'eval_f1_score': 0.6305401662049861, 'eval_precision': 0.6125126135216953, 'eval_recall': 0.6496610774170531, 'eval_roc_auc': 0.6227961648046333, 'eval_matthews_correlation': 0.2459116448092844, 'eval_cohen_kappa': 0.24546700597703708, 'eval_true_negative': 1699, 'eval_false_positive': 1152, 'eval_false_negative': 982, 'eval_true_positive': 1821, 'eval_specificity': 0.5959312521922132, 'eval_sensitivity': 0.6496610774170531, 'eval_informedness': 0.24559232960926636, 'eval_balanced_accuracy': 0.6227961648046332, 'eval_runtime': 48.9888, 'eval_samples_per_second': 115.414, 'eval_steps_per_second': 7.226, 'epoch': 7.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-2600\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-2600\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-2600\\special_tokens_map.json\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2800/3530 [1:19:18<17:34,  1.45s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0262, 'learning_rate': 5.093031306275309e-06, 'epoch': 7.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2800/3530 [1:20:07<17:34,  1.45s/it]Saving model checkpoint to ./model\\checkpoint-2800\n",
      "Configuration saved in ./model\\checkpoint-2800\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 2.319601535797119, 'eval_pred_true': 3078, 'eval_pred_false': 2576, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.623452423063318, 'eval_f1_score': 0.6379867369494983, 'eval_precision': 0.6094866796621182, 'eval_recall': 0.6692829111666072, 'eval_roc_auc': 0.6238382286453871, 'eval_matthews_correlation': 0.24864953273523127, 'eval_cohen_kappa': 0.24747207191640153, 'eval_true_negative': 1649, 'eval_false_positive': 1202, 'eval_false_negative': 927, 'eval_true_positive': 1876, 'eval_specificity': 0.5783935461241669, 'eval_sensitivity': 0.6692829111666072, 'eval_informedness': 0.24767645729077414, 'eval_balanced_accuracy': 0.6238382286453871, 'eval_runtime': 48.5156, 'eval_samples_per_second': 116.54, 'eval_steps_per_second': 7.297, 'epoch': 7.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-2800\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-2800\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-2800\\special_tokens_map.json\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3000/3530 [1:25:03<13:14,  1.50s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.021, 'learning_rate': 2.7298895722308565e-06, 'epoch': 8.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3000/3530 [1:25:51<13:14,  1.50s/it]Saving model checkpoint to ./model\\checkpoint-3000\n",
      "Configuration saved in ./model\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 2.3879358768463135, 'eval_pred_true': 3034, 'eval_pred_false': 2620, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.6248673505482844, 'eval_f1_score': 0.6366284050025698, 'eval_precision': 0.6123928806855636, 'eval_recall': 0.6628612201212986, 'eval_roc_auc': 0.6251871867004247, 'eval_matthews_correlation': 0.2510392328385998, 'eval_cohen_kappa': 0.2502007966339944, 'eval_true_negative': 1675, 'eval_false_positive': 1176, 'eval_false_negative': 945, 'eval_true_positive': 1858, 'eval_specificity': 0.587513153279551, 'eval_sensitivity': 0.6628612201212986, 'eval_informedness': 0.25037437340084967, 'eval_balanced_accuracy': 0.6251871867004248, 'eval_runtime': 48.5938, 'eval_samples_per_second': 116.352, 'eval_steps_per_second': 7.285, 'epoch': 8.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-3000\\special_tokens_map.json\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3200/3530 [1:30:50<08:15,  1.50s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0139, 'learning_rate': 1.0704449843359499e-06, 'epoch': 9.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3200/3530 [1:31:39<08:15,  1.50s/it]Saving model checkpoint to ./model\\checkpoint-3200\n",
      "Configuration saved in ./model\\checkpoint-3200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 2.457062244415283, 'eval_pred_true': 2962, 'eval_pred_false': 2692, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.6248673505482844, 'eval_f1_score': 0.6320901994796184, 'eval_precision': 0.6151249155975692, 'eval_recall': 0.6500178380306814, 'eval_roc_auc': 0.6250790698396129, 'eval_matthews_correlation': 0.250434836722643, 'eval_cohen_kappa': 0.250038742145413, 'eval_true_negative': 1711, 'eval_false_positive': 1140, 'eval_false_negative': 981, 'eval_true_positive': 1822, 'eval_specificity': 0.6001403016485444, 'eval_sensitivity': 0.6500178380306814, 'eval_informedness': 0.25015813967922584, 'eval_balanced_accuracy': 0.6250790698396129, 'eval_runtime': 49.1522, 'eval_samples_per_second': 115.03, 'eval_steps_per_second': 7.202, 'epoch': 9.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-3200\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-3200\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-3200\\special_tokens_map.json\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3400/3530 [1:36:41<03:13,  1.49s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 5654\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0122, 'learning_rate': 1.6713313443384726e-07, 'epoch': 9.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3400/3530 [1:37:30<03:13,  1.49s/it]Saving model checkpoint to ./model\\checkpoint-3400\n",
      "Configuration saved in ./model\\checkpoint-3400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 2) (5654,)\n",
      "(5654,) (5654,)\n",
      "{'eval_loss': 2.4857797622680664, 'eval_pred_true': 2988, 'eval_pred_false': 2666, 'eval_actual_true': 2803, 'eval_actual_false': 2851, 'eval_accuracy': 0.623452423063318, 'eval_f1_score': 0.632360559488862, 'eval_precision': 0.6127844712182061, 'eval_recall': 0.6532286835533357, 'eval_roc_auc': 0.6237030825693721, 'eval_matthews_correlation': 0.24779943172486887, 'eval_cohen_kappa': 0.24726878247098194, 'eval_true_negative': 1694, 'eval_false_positive': 1157, 'eval_false_negative': 972, 'eval_true_positive': 1831, 'eval_specificity': 0.5941774815854086, 'eval_sensitivity': 0.6532286835533357, 'eval_informedness': 0.24740616513874425, 'eval_balanced_accuracy': 0.6237030825693721, 'eval_runtime': 49.3725, 'eval_samples_per_second': 114.517, 'eval_steps_per_second': 7.17, 'epoch': 9.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model\\checkpoint-3400\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\checkpoint-3400\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\checkpoint-3400\\special_tokens_map.json\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3530/3530 [1:40:49<00:00,  1.49s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./model\\checkpoint-600 (score: 0.6757269279393172).\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3530/3530 [1:40:49<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6049.7342, 'train_samples_per_second': 37.383, 'train_steps_per_second': 0.583, 'train_loss': 0.2583849422992458, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3530, training_loss=0.2583849422992458, metrics={'train_runtime': 6049.7342, 'train_samples_per_second': 37.383, 'train_steps_per_second': 0.583, 'train_loss': 0.2583849422992458, 'epoch': 10.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./model/first_model\\config.json\n",
      "Model weights saved in ./model/first_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./model/first_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_wa_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
